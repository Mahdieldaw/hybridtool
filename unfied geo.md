# Unified Geometry Implementation — Provenance & Pruning

**Scope**: Three phases of work that unify the geometry underlying provenance allocation, carrier detection, and pruning. Each phase is independently valuable. No phase requires starting the next. Each phase produces instrumentation that tells you whether the next phase is safe to begin.

**Governing rule**: Do not delete existing calculations in any phase. Existing logic is retained as comparison telemetry until instrumentation proves the new layer is strictly superior.

---

## Phase 1: Statement-Level Competitive Provenance Allocation

### Objective

Add a new statement-level competitive assignment layer inside `reconstructProvenance`. This becomes the canonical evidence allocator. Existing paragraph-based logic remains intact for debug comparison only.

### 1.1 Canonical Claim Embedding

Use claim-text embeddings (the same ones currently generated by `generateClaimEmbeddings`) as the single consistent claim embedding representation.

Do not mix with statement-derived centroids. That decision belongs to Phase 2.

### 1.2 Statement × Claim Similarity Matrix

For every statement S with embedding e_S, for every claim C with embedding e_C:

```
sim(S, C) = cosineSimilarity(e_S, e_C)
```

Store full matrix: `statementSimMatrix[statementId][claimId] = similarity`

This must include ALL statements, not just supporter-filtered ones. Supporter filtering happens after allocation.

### 1.3 Competitive Assignment Per Statement

For each statement S:

- Let `{ s_c }` be similarities across all claims
- Compute `μ_S = mean({ s_c })`
- Compute `σ_S = stddev({ s_c })`
- Define threshold:
  - If number of claims == 2: `τ_S = μ_S`
  - Else: `τ_S = μ_S + σ_S`
- Statement S is assigned to claim C iff `s_c > τ_S`

### 1.4 Evidence Excess and Normalized Weights

For each assigned pair (S, C):

```
excess(S, C) = s_c − τ_S
totalExcess_S = Σ excess(S, C) over assigned claims

If totalExcess_S > 0:
  weight(S, C) = excess(S, C) / totalExcess_S
Else if K assigned claims exist:
  weight(S, C) = 1 / K
```

Constraint: `Σ weight(S, C)` over claims assigned to S = 1. Evidence mass is conserved.

### 1.5 Canonical Provenance Per Claim

For claim C:

- `claimStatementPool` = all statements S where `weight(S, C) > 0`
- Apply supporter constraint: only retain statements where `S.modelIndex ∈ claim.supporters`

Compute:

```
provenanceBulk(C) = Σ weight(S, C) over retained statements
```

Store per-claim:

```
directStatementProvenance = [
  { statementId, similarity, threshold, excess, weight }
]
```

Also compute: poolSize, meanExcess.

### 1.6 Paragraph Aggregation (Derived, Not Canonical)

Paragraph-level evidence is now derived from statement weights:

```
paragraphWeight(P, C) = Σ weight(S, C) for statements S in P
```

This replaces paragraph binary assignment as the canonical signal. Do not remove existing paragraph μ+σ logic. Retain it as debug output for comparison.

### 1.7 Edge Case Guards

- If all `σ_S == 0` across claims: assign each statement equally across all claims.
- If a claim receives zero assigned statements after supporter filtering: retain empty pool. Do not fallback to heuristic top-K.
- No arbitrary similarity thresholds. No fixed top-N selection. All decisions are distribution-relative.

### 1.8 Instrumentation (Critical — gates Phase 2)

**Instrumentation Point 1: Geometry Correlation**

During triage, for each statement involved in a pruning decision, log both:
- `weight(S, C)` from the new competitive allocation
- The relevance score computed by existing pruning logic

Surface the correlation between these two values in the debug panel. This is one number that tells you whether the two systems agree.

**Instrumentation Point 2: Dual Coordinate System Flag**

Log which claim embedding representation was used for each computation. When provenance computes `sim(S, C)` using claim-text embedding and pruning computes relevance using statement-derived centroid, flag this in debug output as:

> "Dual coordinate system active: provenance uses claim-text embeddings, pruning uses statement-derived centroids."

This is not a bug. It is an acknowledged state that Phase 2 resolves.

**Instrumentation Point 3: Competitive Entropy**

For each statement, log how many claims exceed `τ_S`.

- Statements assigned to 1 claim → decisive allocation
- Statements assigned to many claims → ambiguous

Surface the distribution of assignment counts across all statements. If every statement is assigned to every claim, competitive allocation is not differentiating.

**Instrumentation Point 4: Old vs. New Comparison**

For each claim, compare:
- Paragraph-based pool size (old system)
- Statement-derived paragraph aggregation (new system)
- Overlap ratio between the two pools

Surface disagreement statistics in debug panel.

### Phase 1 Completion Criteria

- Statement-level competitive allocation produces nonzero exclusivity
- Competitive entropy distribution shows meaningful variation (not all statements assigned to all claims, not all assigned to exactly one)
- Geometry correlation (instrumentation point 1) is measurable
- Old vs. new comparison shows where the systems agree and diverge

---

## Phase 2: Canonical Embedding Resolution + Continuous Field

**Gate**: Do not begin until Phase 1 instrumentation data is reviewed. Specifically:
- Read the geometry correlation from Instrumentation Point 1
- Read the dual coordinate system divergence patterns
- Decide canonical embedding based on evidence, not assumption

### 2.1 Canonical Embedding Decision

Phase 1 logs both claim-text embeddings and statement-derived centroids in parallel. Review the divergence:

- If the two representations produce similar competitive allocations (high correlation from Instrumentation Point 1): pick whichever is computationally cheaper.
- If they diverge significantly: understand why before choosing. The divergence pattern tells you which representation better captures the claim's actual evidence footprint.

Move the chosen canonical embedding upstream so both provenance and pruning share the same representation. Kill the dual coordinate system flag.

### 2.2 Continuous Per-Claim Relevance Field

This is orthogonal to competitive allocation. It answers a different question.

- Competitive allocation asks: "Which claims compete for this statement?" (cross-claim)
- Continuous field asks: "How deeply does this statement sit inside a claim's semantic gravity well?" (within-claim)

For each claim C, using the canonical embedding:

```
For every statement S:
  sim_claim(S) = cosine(e_C, e_S)

μ_C = mean(sim_claim over all statements)
σ_C = stddev(sim_claim over all statements)

z_claim(S) = (sim_claim(S) − μ_C) / σ_C
```

### 2.3 Core Cluster Identification

Define core set for claim C as statements where `z_claim > 1.0` (top of distribution, data-relative).

For every statement S:

```
sim_core(S) = mean cosine similarity to core set statements
```

Standardize:

```
z_core(S) = (sim_core(S) − μ_core) / σ_core
```

### 2.4 Continuous Evidence Score

```
evidenceScore(S) = z_claim(S) + z_core(S)
```

This is continuous, relative, and threshold-free. Store as:

```
claim.continuousField = [
  { statementId, sim_claim, z_claim, z_core, evidenceScore }
]
```

### 2.5 Disagreement Matrix (The Gold)

Cross-reference competitive allocation (Phase 1) against continuous field (Phase 2).

Flag statements where:
- Competitive allocation assigns to Claim A (highest weight)
- But continuous field evidenceScore is highest for Claim B

These are the structural fault lines where pruning decisions are most likely to be wrong. A statement can be competitively assigned to one claim but sit in the dense core of another.

Surface this disagreement matrix in debug panel. It tells you where the current pruning logic would make the wrong call.

### Phase 2 Completion Criteria

- Single canonical embedding used by both provenance and pruning
- Continuous field produces meaningful evidenceScore variation per claim
- Disagreement matrix identifies specific statements where competitive allocation and continuous field diverge
- Patterns in the disagreement are understood (not just measured)

---

## Phase 3: Pruning Refactor

**Gate**: Do not begin until Phase 2 disagreement patterns are understood. Specifically:
- The geometry correlation (Phase 1) should be high enough that refactoring won't break things
- The disagreement matrix (Phase 2) should be understood well enough to predict impact

### 3.1 Replace Relevance Gating

**Current**: Gather all source-to-centroid similarities across pruned claims. Sort. Find elbow. `dynamicRelevanceMin = elbow value`, fallback 0.55.

**Replace with**: A statement's relevance to a pruned claim is its competitive `weight(S, C)` from the allocation layer.

Statements with zero weight for the pruned claim are not relevant. Statements with high weight are strongly relevant. No elbow. No 0.55 fallback.

The 0.55 is a magic number. Kill it.

### 3.2 Replace Carrier Detection Thresholds

**Current**: `threshold = max(μ+σ, P75)` computed from per-pruned-claim local distribution.

**Replace with**: A surviving statement "carries" a pruned statement if the pruned statement's competitive weight for the surviving claim is high — meaning the surviving claim already owns that evidence mass.

Specifically:
- For a pruned statement S assigned to pruned claim C_pruned:
- Check: does S have positive competitive weight for any surviving claim C_surviving?
- If `weight(S, C_surviving) > weight(S, C_pruned)`: strong carrier. The surviving claim owns this statement more than the pruned claim did.
- If `weight(S, C_surviving) > 0` but `< weight(S, C_pruned)`: weak carrier. The surviving claim has partial ownership.
- If `weight(S, C_surviving) == 0`: no carrier. This evidence is exclusive to the pruned claim.

This replaces the static 0.60 threshold and the 82% pass rate problem. Carrier detection becomes a competitive weight comparison, not a cosine threshold.

### 3.3 Remove Magic Numbers

After refactor, the following hardcoded values should be eliminated:
- `0.55` relevance gating fallback
- `0.60` carrier detection static threshold
- Any fixed top-K selection in provenance

All decisions are now distribution-relative through competitive allocation.

### 3.4 Continuous Field Integration (Optional)

If Phase 2's disagreement matrix reveals cases where competitive allocation is wrong but continuous field is right, integrate evidenceScore as a secondary signal:

- A statement is prunable only if:
  - Competitive weight for the pruned claim is low
  - AND evidenceScore for the pruned claim is low
  - AND it has positive weight or evidenceScore for at least one surviving claim

This becomes a global optimization rather than per-claim threshold gating. Implement only if Phase 2 evidence warrants it.

### Phase 3 Completion Criteria

- No magic cosine thresholds remain in pruning
- Carrier pass rate drops from 82% to a meaningful range
- Pruning and provenance share the same geometric foundation
- Isolated statements (high exclusivity for pruned claim, zero weight for surviving claims) are protected by the Irreplaceability Principle — they cannot be silently removed

---

## What This Does Not Touch

- **Basin Inversion Diagnostic View**: Separate instrument. Describes global similarity topology. Orthogonal to provenance.
- **Mutual Recognition Graph / Regionization**: Separate structural layer. Regions feed model ordering and coverage audit. Provenance feeds claim-evidence ownership. They share the embedding space but answer different questions.
- **Blast Radius Weights**: The policy blend (0.30/0.25/0.20/0.15/0.10) is unchanged. Blast radius consumes provenance outputs — once provenance improves, blast radius activates without code changes.
- **Mapper**: No changes to mapper prompt construction, claim extraction, or edge labeling.

---

## Summary Table

| Phase | What Changes | What It Measures | Gate for Next Phase |
|-------|-------------|-----------------|-------------------|
| 1 | Statement-level competitive allocation added alongside existing paragraph logic | Whether competitive weights differentiate claims; whether they agree with existing pruning relevance | Geometry correlation + competitive entropy |
| 2 | Canonical embedding chosen; continuous per-claim field added | Within-claim semantic density; disagreement between competitive and continuous signals | Disagreement matrix understood |
| 3 | Pruning refactored to use competitive weights instead of local thresholds | Whether unified geometry improves pruning precision; carrier pass rate | Phase 2 evidence warrants it |




followup:
# Mixed-Method Provenance: Implementation Specification

## Context

We currently compute paragraph-level competitive provenance allocation in `reconstructProvenance`. Each paragraph computes its similarity to all claim centroids and assigns itself to claims exceeding its own μ+σ threshold. This is **paragraph-centric competitive allocation** — the paragraph decides which claims deserve it.

We are adding a second, complementary allocation: **claim-centric direct scoring** — each claim scores all paragraphs by direct cosine similarity, thresholded by the claim's own distribution. The two pools are unioned, then statement-level direct cosine within the merged pool removes paragraph-inherited noise.

This does not replace existing competitive allocation. It runs alongside it. Both feed into a merged pool. Existing competitive logic is retained intact.

## Why

Competitive allocation is blind to evidence that another claim "won" — if paragraph P scores μ+σ for claim_4 but only μ+0.8σ for claim_2, claim_2 never sees P even if P contains the most semantically relevant statement in the entire corpus for claim_2. Claim-centric scoring recovers these cases because it asks "what's close to this claim?" not "who wins this paragraph?"

Neither method alone is faithful. Their failure modes are structurally different and non-overlapping. The union has strictly better recall. Statement-level filtering restores precision.

---

## Step 1: Claim-Centric Paragraph Scoring

### Location
Add inside `reconstructProvenance`, after existing paragraph competitive allocation is computed. Do not modify existing competitive logic.

### Computation

```
For each claim C with centroid embedding e_C:
  For each paragraph P with embedding e_P:
    claimCentricSim[C][P] = cosineSimilarity(e_P, e_C)
  
  μ_C = mean(claimCentricSim[C][P] for all P)
  σ_C = stddev(claimCentricSim[C][P] for all P)
  threshold_C = μ_C + σ_C
  
  claimCentricPool[C] = all P where claimCentricSim[C][P] > threshold_C
```

### Edge cases
- If σ_C == 0 (all paragraphs equidistant from claim): claimCentricPool[C] = empty. This claim has no distinguishable paragraph affinity. Do not fallback.
- If number of paragraphs < 3: skip claim-centric, rely on competitive only.

### Store per claim
```
claimCentricProvenance[C] = {
  paragraphs: [ { paragraphId, sim, threshold, aboveThreshold: boolean } ],
  μ: number,
  σ: number,
  threshold: number,
  poolSize: number
}
```

Store the FULL ranked list of all paragraphs with their sim scores, not just the ones above threshold. The threshold determines the pool but the full ranking is needed for the debug panel.

---

## Step 2: Merge Pools

### Computation

```
For each claim C:
  competitivePool[C] = existing paragraph pool from current competitive allocation
  claimCentricPool[C] = from Step 1
  
  mergedParagraphPool[C] = union(competitivePool[C], claimCentricPool[C])
```

Deduplicate by paragraph ID.

### Store per claim
```
mergedProvenance[C] = {
  paragraphIds: string[],
  fromCompetitiveOnly: string[],    // in competitive but not claim-centric
  fromClaimCentricOnly: string[],   // in claim-centric but not competitive
  fromBoth: string[],               // in both pools
  totalMerged: number
}
```

The three subsets (competitiveOnly, claimCentricOnly, both) are the primary diagnostic. They show where the methods agree and diverge.

---

## Step 3: Statement-Level Refinement Within Merged Pool

### Computation

CRITICAL: The μ and σ used for statement classification must be computed over ALL statements in the corpus, not just the union pool. The union pool is already filtered for paragraph-level proximity — computing statistics only within it inflates μ and compresses σ, causing the threshold to rubber-stamp everything.

```
For each claim C:
  // Step A: Compute GLOBAL distribution of all statements against this claim
  For ALL statements S in the full corpus:
    globalSim[S] = cosineSimilarity(e_S, e_C)   // statement embedding to claim centroid
  
  μ_global = mean(globalSim over ALL statements)
  σ_global = stddev(globalSim over ALL statements)
  
  // Step B: Identify candidate statements from merged paragraph pool
  candidateStatements = all statements S where S.paragraphId ∈ mergedParagraphPool[C]
  
  // Step C: Classify candidates using GLOBAL thresholds
  For each candidate statement S:
    if globalSim[S] > μ_global + σ_global:
      fate = "primary"
    else if globalSim[S] > μ_global:
      fate = "supporting"
    else:
      fate = "inherited"
```

This ensures a statement is classified "primary" only if it is notably closer to this claim than the average statement in the entire corpus — not just better than other pre-filtered neighbors.

### Apply supporter constraint
After statement classification, filter: only retain statements where S.modelIndex ∈ claim.supporters. Statements from non-supporting models are excluded regardless of geometric proximity.

### Store per claim
```
statementRefinement[C] = {
  statements: [
    { 
      statementId, 
      directSim,           // raw cosine to claim centroid (same as globalSim)
      fate,                 // "primary" | "supporting" | "inherited"
      μ_global,             // corpus-wide mean for this claim (for context)
      σ_global,             // corpus-wide stddev for this claim
      fromParagraphId,     // which paragraph brought this statement in
      paragraphSource,     // "competitive" | "claimCentric" | "both"
      competitiveWeight,   // from existing §1 allocation if available, null otherwise
      continuousScore      // from §2 continuous field if available, null otherwise
    }
  ],
  primaryCount: number,
  supportingCount: number,
  inheritedCount: number
}
```

---

## Step 4: Continuous Field as Diagnostic Overlay

If Phase 2 continuous field (evidenceScore = z_claim + z_core) is already computed, cross-reference it against statement fates from Step 3.

### Flag disagreements

```
For each statement S in claim C's refined pool:
  if fate == "inherited" AND continuousScore > 0:
    flag = "continuous_disagrees_with_exclusion"
  
  if fate == "primary" AND continuousScore < 0:
    flag = "continuous_disagrees_with_inclusion"
  
  if fate == "primary" AND competitiveWeight == 0:
    flag = "competitive_disagrees_with_inclusion"
```

These flags are diagnostic only. They do not change fates. They surface in the debug panel for manual inspection.

---

## Debug Panel Output

### New section: "Mixed Provenance" per claim

**Header row**: `{claimLabel} — {primaryCount} primary · {supportingCount} supporting · {inheritedCount} inherited · {mergedParagraphCount} paragraphs ({fromBoth}∩ {fromCompetitiveOnly}C {fromClaimCentricOnly}CC)`

**Three sub-views:**

#### Sub-view 1: Paragraph Sources
Show all paragraphs in merged pool, sorted by claimCentricSim descending.

| Column | Value |
|---|---|
| Paragraph ID | ID |
| Claim-centric sim | cosine to claim centroid |
| Source | "both" / "competitive only" / "claim-centric only" |
| Statements contained | count of primary/supporting/inherited within this paragraph |
| Paragraph text | truncated first 120 chars |

Highlight rows where source = "claim-centric only" — these are the recovered paragraphs competitive missed.

#### Sub-view 2: Statement Fates
Show all candidate statements sorted by directSim descending.

| Column | Value |
|---|---|
| Statement ID | ID |
| Direct sim | cosine to claim centroid |
| Fate | primary / supporting / inherited |
| Paragraph source | which pool brought its paragraph in |
| Competitive weight | from §1 if available |
| Continuous score | from §2 if available |
| Flags | any disagreement flags from Step 4 |
| Text | truncated first 120 chars |

Color coding: green = primary, amber = supporting, grey = inherited, red outline = any disagreement flag.

#### Sub-view 3: Method Comparison (existing Compare panel columns)
Retain the existing three-column comparison (Direct / Competitive / Continuous) from the current panel. The new mixed provenance lives alongside it, not replacing it. The user can compare what mixed provenance chose against what each individual method would have chosen.

---

## What This Does NOT Change

- **Existing competitive paragraph allocation**: Retained intact. It feeds into the merge as one of two inputs.
- **Existing §1 statement-level competitive allocation**: Retained intact. Its weights appear as a diagnostic column in the statement fates view.
- **Existing §2 continuous field**: Retained intact. Its evidenceScore appears as a diagnostic column.
- **Blast radius**: Consumes provenance outputs. Once merged provenance improves source statement pools, blast radius benefits automatically without code changes.
- **Mapper**: No changes.
- **Traversal / Survey**: No changes.

## What This DOES Change

- **Canonical provenance pool per claim**: Now derived from merged paragraph pool + statement refinement, not from competitive paragraph allocation alone.
- **Skeletonization input**: When triage consumes source statement IDs for a claim, it should consume the "primary" + "supporting" statements from mixed provenance rather than the old competitive-only pool. Inherited statements are excluded from skeletonization consideration.
- **Exclusivity calculation**: Recomputed on the merged pools. A statement is exclusive to claim C if it appears as "primary" or "supporting" in C's pool and does not appear as "primary" or "supporting" in any other claim's pool.

---

## Completion Criteria

1. Claim-centric paragraph scoring produces ranked lists for all claims with visible thresholds
2. Merged pools are strictly larger than or equal to competitive-only pools (union can only add, never subtract)
3. Statement refinement produces meaningful three-tier distribution (not all primary, not all inherited)
4. Debug panel surfaces all three sub-views with disagreement flags
5. At least one claim shows a "claim-centric only" paragraph that contains a statement scored as "primary" after refinement — this is the recovered evidence case that motivated the entire change
6. Existing competitive allocation, continuous field, and Compare panel remain functional and unchanged

---

## Instrumentation for Future Decision

After implementation, surface these aggregate statistics across all claims:

- **Recovery rate**: What percentage of "primary" statements came from claim-centric-only paragraphs? (How much evidence was competitive missing?)
- **Noise rate**: What percentage of "inherited" statements came from claim-centric-only paragraphs? (How much noise did the wider net introduce before statement filtering caught it?)
- **Agreement rate**: What percentage of merged paragraphs were in both pools? (How often do the methods agree?)
- **Disagreement flag rate**: What percentage of statements have any Step 4 disagreement flag? (How often do the diagnostic overlays disagree with the primary derivation?)

These four numbers tell you whether the mixed method is worth keeping, whether statement filtering is doing its job, and whether the continuous/competitive diagnostics reveal patterns worth acting on.