# Unified Geometry Implementation — Provenance & Pruning

**Scope**: Three phases of work that unify the geometry underlying provenance allocation, carrier detection, and pruning. Each phase is independently valuable. No phase requires starting the next. Each phase produces instrumentation that tells you whether the next phase is safe to begin.

**Governing rule**: Do not delete existing calculations in any phase. Existing logic is retained as comparison telemetry until instrumentation proves the new layer is strictly superior.

---

## Phase 1: Statement-Level Competitive Provenance Allocation

### Objective

Add a new statement-level competitive assignment layer inside `reconstructProvenance`. This becomes the canonical evidence allocator. Existing paragraph-based logic remains intact for debug comparison only.

### 1.1 Canonical Claim Embedding

Use claim-text embeddings (the same ones currently generated by `generateClaimEmbeddings`) as the single consistent claim embedding representation.

Do not mix with statement-derived centroids. That decision belongs to Phase 2.

### 1.2 Statement × Claim Similarity Matrix

For every statement S with embedding e_S, for every claim C with embedding e_C:

```
sim(S, C) = cosineSimilarity(e_S, e_C)
```

Store full matrix: `statementSimMatrix[statementId][claimId] = similarity`

This must include ALL statements, not just supporter-filtered ones. Supporter filtering happens after allocation.

### 1.3 Competitive Assignment Per Statement

For each statement S:

- Let `{ s_c }` be similarities across all claims
- Compute `μ_S = mean({ s_c })`
- Compute `σ_S = stddev({ s_c })`
- Define threshold:
  - If number of claims == 2: `τ_S = μ_S`
  - Else: `τ_S = μ_S + σ_S`
- Statement S is assigned to claim C iff `s_c > τ_S`

### 1.4 Evidence Excess and Normalized Weights

For each assigned pair (S, C):

```
excess(S, C) = s_c − τ_S
totalExcess_S = Σ excess(S, C) over assigned claims

If totalExcess_S > 0:
  weight(S, C) = excess(S, C) / totalExcess_S
Else if K assigned claims exist:
  weight(S, C) = 1 / K
```

Constraint: `Σ weight(S, C)` over claims assigned to S = 1. Evidence mass is conserved.

### 1.5 Canonical Provenance Per Claim

For claim C:

- `claimStatementPool` = all statements S where `weight(S, C) > 0`
- Apply supporter constraint: only retain statements where `S.modelIndex ∈ claim.supporters`

Compute:

```
provenanceBulk(C) = Σ weight(S, C) over retained statements
```

Store per-claim:

```
directStatementProvenance = [
  { statementId, similarity, threshold, excess, weight }
]
```

Also compute: poolSize, meanExcess.

### 1.6 Paragraph Aggregation (Derived, Not Canonical)

Paragraph-level evidence is now derived from statement weights:

```
paragraphWeight(P, C) = Σ weight(S, C) for statements S in P
```

This replaces paragraph binary assignment as the canonical signal. Do not remove existing paragraph μ+σ logic. Retain it as debug output for comparison.

### 1.7 Edge Case Guards

- If all `σ_S == 0` across claims: assign each statement equally across all claims.
- If a claim receives zero assigned statements after supporter filtering: retain empty pool. Do not fallback to heuristic top-K.
- No arbitrary similarity thresholds. No fixed top-N selection. All decisions are distribution-relative.

### 1.8 Instrumentation (Critical — gates Phase 2)

**Instrumentation Point 1: Geometry Correlation**

During triage, for each statement involved in a pruning decision, log both:
- `weight(S, C)` from the new competitive allocation
- The relevance score computed by existing pruning logic

Surface the correlation between these two values in the debug panel. This is one number that tells you whether the two systems agree.

**Instrumentation Point 2: Dual Coordinate System Flag**

Log which claim embedding representation was used for each computation. When provenance computes `sim(S, C)` using claim-text embedding and pruning computes relevance using statement-derived centroid, flag this in debug output as:

> "Dual coordinate system active: provenance uses claim-text embeddings, pruning uses statement-derived centroids."

This is not a bug. It is an acknowledged state that Phase 2 resolves.

**Instrumentation Point 3: Competitive Entropy**

For each statement, log how many claims exceed `τ_S`.

- Statements assigned to 1 claim → decisive allocation
- Statements assigned to many claims → ambiguous

Surface the distribution of assignment counts across all statements. If every statement is assigned to every claim, competitive allocation is not differentiating.

**Instrumentation Point 4: Old vs. New Comparison**

For each claim, compare:
- Paragraph-based pool size (old system)
- Statement-derived paragraph aggregation (new system)
- Overlap ratio between the two pools

Surface disagreement statistics in debug panel.

### Phase 1 Completion Criteria

- Statement-level competitive allocation produces nonzero exclusivity
- Competitive entropy distribution shows meaningful variation (not all statements assigned to all claims, not all assigned to exactly one)
- Geometry correlation (instrumentation point 1) is measurable
- Old vs. new comparison shows where the systems agree and diverge

---

## Phase 2: Canonical Embedding Resolution + Continuous Field

**Gate**: Do not begin until Phase 1 instrumentation data is reviewed. Specifically:
- Read the geometry correlation from Instrumentation Point 1
- Read the dual coordinate system divergence patterns
- Decide canonical embedding based on evidence, not assumption

### 2.1 Canonical Embedding Decision

Phase 1 logs both claim-text embeddings and statement-derived centroids in parallel. Review the divergence:

- If the two representations produce similar competitive allocations (high correlation from Instrumentation Point 1): pick whichever is computationally cheaper.
- If they diverge significantly: understand why before choosing. The divergence pattern tells you which representation better captures the claim's actual evidence footprint.

Move the chosen canonical embedding upstream so both provenance and pruning share the same representation. Kill the dual coordinate system flag.

### 2.2 Continuous Per-Claim Relevance Field

This is orthogonal to competitive allocation. It answers a different question.

- Competitive allocation asks: "Which claims compete for this statement?" (cross-claim)
- Continuous field asks: "How deeply does this statement sit inside a claim's semantic gravity well?" (within-claim)

For each claim C, using the canonical embedding:

```
For every statement S:
  sim_claim(S) = cosine(e_C, e_S)

μ_C = mean(sim_claim over all statements)
σ_C = stddev(sim_claim over all statements)

z_claim(S) = (sim_claim(S) − μ_C) / σ_C
```

### 2.3 Core Cluster Identification

Define core set for claim C as statements where `z_claim > 1.0` (top of distribution, data-relative).

For every statement S:

```
sim_core(S) = mean cosine similarity to core set statements
```

Standardize:

```
z_core(S) = (sim_core(S) − μ_core) / σ_core
```

### 2.4 Continuous Evidence Score

```
evidenceScore(S) = z_claim(S) + z_core(S)
```

This is continuous, relative, and threshold-free. Store as:

```
claim.continuousField = [
  { statementId, sim_claim, z_claim, z_core, evidenceScore }
]
```

### 2.5 Disagreement Matrix (The Gold)

Cross-reference competitive allocation (Phase 1) against continuous field (Phase 2).

Flag statements where:
- Competitive allocation assigns to Claim A (highest weight)
- But continuous field evidenceScore is highest for Claim B

These are the structural fault lines where pruning decisions are most likely to be wrong. A statement can be competitively assigned to one claim but sit in the dense core of another.

Surface this disagreement matrix in debug panel. It tells you where the current pruning logic would make the wrong call.

### Phase 2 Completion Criteria

- Single canonical embedding used by both provenance and pruning
- Continuous field produces meaningful evidenceScore variation per claim
- Disagreement matrix identifies specific statements where competitive allocation and continuous field diverge
- Patterns in the disagreement are understood (not just measured)

---

## Phase 3: Pruning Refactor

**Gate**: Do not begin until Phase 2 disagreement patterns are understood. Specifically:
- The geometry correlation (Phase 1) should be high enough that refactoring won't break things
- The disagreement matrix (Phase 2) should be understood well enough to predict impact

### 3.1 Replace Relevance Gating

**Current**: Gather all source-to-centroid similarities across pruned claims. Sort. Find elbow. `dynamicRelevanceMin = elbow value`, fallback 0.55.

**Replace with**: A statement's relevance to a pruned claim is its competitive `weight(S, C)` from the allocation layer.

Statements with zero weight for the pruned claim are not relevant. Statements with high weight are strongly relevant. No elbow. No 0.55 fallback.

The 0.55 is a magic number. Kill it.

### 3.2 Replace Carrier Detection Thresholds

**Current**: `threshold = max(μ+σ, P75)` computed from per-pruned-claim local distribution.

**Replace with**: A surviving statement "carries" a pruned statement if the pruned statement's competitive weight for the surviving claim is high — meaning the surviving claim already owns that evidence mass.

Specifically:
- For a pruned statement S assigned to pruned claim C_pruned:
- Check: does S have positive competitive weight for any surviving claim C_surviving?
- If `weight(S, C_surviving) > weight(S, C_pruned)`: strong carrier. The surviving claim owns this statement more than the pruned claim did.
- If `weight(S, C_surviving) > 0` but `< weight(S, C_pruned)`: weak carrier. The surviving claim has partial ownership.
- If `weight(S, C_surviving) == 0`: no carrier. This evidence is exclusive to the pruned claim.

This replaces the static 0.60 threshold and the 82% pass rate problem. Carrier detection becomes a competitive weight comparison, not a cosine threshold.

### 3.3 Remove Magic Numbers

After refactor, the following hardcoded values should be eliminated:
- `0.55` relevance gating fallback
- `0.60` carrier detection static threshold
- Any fixed top-K selection in provenance

All decisions are now distribution-relative through competitive allocation.

### 3.4 Continuous Field Integration (Optional)

If Phase 2's disagreement matrix reveals cases where competitive allocation is wrong but continuous field is right, integrate evidenceScore as a secondary signal:

- A statement is prunable only if:
  - Competitive weight for the pruned claim is low
  - AND evidenceScore for the pruned claim is low
  - AND it has positive weight or evidenceScore for at least one surviving claim

This becomes a global optimization rather than per-claim threshold gating. Implement only if Phase 2 evidence warrants it.

### Phase 3 Completion Criteria

- No magic cosine thresholds remain in pruning
- Carrier pass rate drops from 82% to a meaningful range
- Pruning and provenance share the same geometric foundation
- Isolated statements (high exclusivity for pruned claim, zero weight for surviving claims) are protected by the Irreplaceability Principle — they cannot be silently removed

---

## What This Does Not Touch

- **Basin Inversion Diagnostic View**: Separate instrument. Describes global similarity topology. Orthogonal to provenance.
- **Mutual Recognition Graph / Regionization**: Separate structural layer. Regions feed model ordering and coverage audit. Provenance feeds claim-evidence ownership. They share the embedding space but answer different questions.
- **Blast Radius Weights**: The policy blend (0.30/0.25/0.20/0.15/0.10) is unchanged. Blast radius consumes provenance outputs — once provenance improves, blast radius activates without code changes.
- **Mapper**: No changes to mapper prompt construction, claim extraction, or edge labeling.

---

## Summary Table

| Phase | What Changes | What It Measures | Gate for Next Phase |
|-------|-------------|-----------------|-------------------|
| 1 | Statement-level competitive allocation added alongside existing paragraph logic | Whether competitive weights differentiate claims; whether they agree with existing pruning relevance | Geometry correlation + competitive entropy |
| 2 | Canonical embedding chosen; continuous per-claim field added | Within-claim semantic density; disagreement between competitive and continuous signals | Disagreement matrix understood |
| 3 | Pruning refactored to use competitive weights instead of local thresholds | Whether unified geometry improves pruning precision; carrier pass rate | Phase 2 evidence warrants it |